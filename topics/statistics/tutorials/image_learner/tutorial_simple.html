<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gleam Image Learner - Validating Skin Lesion Classification on HAM10000</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            color: #333;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 3px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 5px;
        }
        a {
            color: #0066cc;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
        }
        .comment {
            background-color: #e7f3ff;
            padding: 10px;
            border-left: 4px solid #0066cc;
            margin: 15px 0;
        }
        .tip {
            background-color: #f0f9ff;
            padding: 10px;
            border-left: 4px solid #00aa00;
            margin: 15px 0;
        }
    </style>
</head>
<body>

<h1>Gleam Image Learner - Validating Skin Lesion Classification on HAM10000</h1>

<div class="comment">
<strong>Image Learner Tool</strong>
<p>The Image Learner tool described in this tutorial is currently available on:</p>
<ul>
<li><a href="https://usegalaxy.org">Galaxy US Server</a> - Statistics and Visualization > Machine Learning > Image Learner</li>
<li><a href="https://cancer.usegalaxy.org">Cancer-Galaxy</a> - Galaxy Learning and Modeling tools > Image Learner</li>
</ul>
</div>

<p>In this tutorial, we will use the HAM10000 ("Human Against Machine with 10,000 training images") dataset to develop a deep learning classifier for dermoscopic skin lesion classification. The goal is to accurately classify seven types of pigmented skin lesions using the GLEAM Image Learner tool.</p>

<p>To achieve this, we will follow three essential steps: (i) upload the HAM10000 images and metadata to Galaxy, (ii) set up and run the Image Learner tool to train a deep learning model, and (iii) evaluate the model's predictive performance by analyzing key performance metrics such as accuracy, weighted precision/recall/F1, ROC-AUC, and confusion matrices.</p>

<div class="comment">
<strong>Background</strong>
<p>The <a href="https://zenodo.org/records/17114688">HAM10000 dataset</a> is a preprocessed, balanced subset of the original HAM10000 collection, following the methodology described by Shetty et al. (2022). The dataset covers seven types of pigmented skin lesions:</p>

<ol>
<li>Melanoma (mel)</li>
<li>Melanocytic nevus (nv)</li>
<li>Basal cell carcinoma (bcc)</li>
<li>Actinic keratosis (akiec)</li>
<li>Benign keratosis (bkl)</li>
<li>Dermatofibroma (df)</li>
<li>Vascular lesion (vasc)</li>
</ol>

<p>To address class imbalance in the original dataset (where melanocytic nevi represented 67% of images), we applied preprocessing steps including image resizing to 96×96 pixels, selecting 100 images per class, and applying horizontal flip augmentation to generate 200 balanced images per class (1,400 total images).</p>
</div>

<h2>Dataset Preprocessing and Balanced Composition</h2>

<p>The preprocessed dataset comprises:</p>
<ul>
<li><strong>Total Images</strong>: 1,400 dermoscopic images (balanced)</li>
<li><strong>Image Size</strong>: 96×96 pixels</li>
<li><strong>Image Format</strong>: PNG</li>
<li><strong>Lesion Types</strong>: 7 categories</li>
<li><strong>Per Class</strong>: 200 images each</li>
</ul>

<h3>Preprocessing Steps</h3>
<p>Starting from the original HAM10000 dataset (10,015 images with severe class imbalance), we applied:</p>
<ol>
<li><strong>Image Selection</strong>: Selected 100 images per class</li>
<li><strong>Image Resizing</strong>: Resized to 96×96 pixels</li>
<li><strong>Data Augmentation</strong>: Applied horizontal flip to generate 200 images per class</li>
</ol>

<h3>Balanced Class Distribution</h3>

<table>
<tr>
<th>Lesion Type</th>
<th>Images</th>
<th>Percentage</th>
</tr>
<tr>
<td>Melanocytic nevus (nv)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Melanoma (mel)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Basal cell carcinoma (bcc)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Actinic keratosis (akiec)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Benign keratosis (bkl)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Dermatofibroma (df)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td>Vascular lesion (vasc)</td>
<td>200</td>
<td>14.3%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>1,400</strong></td>
<td><strong>100%</strong></td>
</tr>
</table>

<p>This balanced dataset allows the Image Learner model to learn effectively from all lesion types without bias toward any majority class.</p>

<h2>Data Augmentation</h2>

<p>Following Shetty et al. (2022), <strong>horizontal flip augmentation</strong> was applied during dataset preparation. This transformation:</p>
<ul>
<li>Improves robustness to lesion orientation and acquisition variability</li>
<li>Increases effective training diversity without collecting additional images</li>
<li>Helps reduce sensitivity to class- and pose-specific patterns</li>
<li>Preserves diagnostically relevant structures while introducing harmless variation</li>
</ul>

<p><em>Reference: Shetty, B., Fernandes, R., Rodrigues, A.P. et al. (2022). Skin lesion classification of dermoscopic images using machine learning and convolutional neural network. Scientific Reports 12, 18134. <a href="https://www.nature.com/articles/s41598-022-22644-9">https://doi.org/10.1038/s41598-022-22644-9</a></em></p>

<h2>Model Configuration in GLEAM Image Learner</h2>

<p>After uploading the dataset, configure the Image Learner parameters as follows:</p>

<table>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Rationale</th>
</tr>
<tr>
<td>Task Type</td>
<td>Multi-class Classification</td>
<td>Seven lesion classes</td>
</tr>
<tr>
<td>Model Name</td>
<td>caformer_s18_384</td>
<td>Efficient transformer-based model</td>
</tr>
<tr>
<td>Customize Default Settings</td>
<td>Yes</td>
<td>Expose epochs and batch size settings</td>
</tr>
<tr>
<td>Epochs</td>
<td>30</td>
<td>Sufficient for convergence without overfitting</td>
</tr>
<tr>
<td>Define your batch size</td>
<td>Yes</td>
<td>Set a stable batch size for training</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>Balances memory and gradient stability</td>
</tr>
<tr>
<td>Data Split</td>
<td>Stratified 70/10/20</td>
<td>Train/validation/test split</td>
</tr>
</table>

<h2>Prepare Environment and Get the Data</h2>

<div class="comment">
<strong>Dataset Preprocessing</strong>
<p>The dataset available on Zenodo has been preprocessed following Shetty et al. (2022) methodology.</p>
</div>

<h3>Environment and Data Upload</h3>

<ol>
<li>Create a new history for this tutorial. You can name it <em>HAM10000 Image Classification</em>.</li>

<li>Import the dataset files from Zenodo:
<pre><code>https://zenodo.org/records/17114688/files/images_96.zip
https://zenodo.org/records/17114688/files/image_metadata_new.csv</code></pre>
</li>

<li>Check that the data formats are assigned correctly:
<ul>
<li>The <code>.zip</code> file should have type <code>zip</code></li>
<li>The <code>.csv</code> file should have type <code>tabular</code></li>
</ul>
</li>

<li>Add tags to the datasets for better organization:
<ul>
<li>Add tag <code>HAM10000 images</code> to the images_96.zip file</li>
<li>Add tag <code>HAM10000 metadata</code> to the image_metadata_new.csv file</li>
</ul>
</li>
</ol>

<h2>Using Image Learner Tool</h2>

<ol>
<li>Run the <strong>Image Learner</strong> tool with the following parameters:
<ul>
<li><strong>The metadata csv containing image_path column, label column</strong>: <code>image_metadata_new.csv</code></li>
<li><strong>Image zip</strong>: <code>images_96.zip</code></li>
<li><strong>Task Type</strong>: <code>Multi-class Classification</code></li>
<li><strong>Select a model for your experiment</strong>: <code>caformer_s18_384</code></li>
<li><strong>Customize Default Settings</strong>: <code>Yes</code></li>
<li><strong>Epochs</strong>: <code>30</code></li>
<li><strong>Define your batch size</strong>: <code>Yes</code></li>
<li><strong>Batch Size</strong>: <code>32</code></li>
</ul>
</li>

<li>Run the tool</li>
</ol>

<h2>Tool Output Files</h2>

<p>After training and testing your model, you should see several new files in your history list:</p>

<ul>
<li><strong>Image Learner Trained Model (ludwig_model)</strong>: A reusable model bundle including configuration JSONs and model weights.</li>
<li><strong>Image Learner Model Report (HTML)</strong>: An interactive report summarizing configuration, metrics, and plots.</li>
<li><strong>Image Learner Predictions/Stats/Plots (collection)</strong>:
<ul>
<li><code>predictions.csv</code> with model predictions and confidence scores</li>
<li>JSON files with experiment metadata and metrics (for example <code>training_statistics.json</code>, <code>test_statistics.json</code>)</li>
<li>PNG plots from training and test visualizations, plus feature-importance examples</li>
<li><code>feature_importance_examples.zip</code> bundling feature-importance images</li>
</ul>
</li>
</ul>

<h2>Model Performance</h2>

<p>The Image Learner report provides a complete overview of configuration, learning curves, and test diagnostics including confusion matrices and per-class metrics. On the balanced split, the model achieves approximately 90% accuracy with weighted precision/recall/F1 around 0.90. The report also includes ROC-AUC and Cohen's Kappa for additional discrimination and agreement context.</p>

<h3>Comparison with Shetty et al. (2022)</h3>

<table>
<tr>
<th>Metric</th>
<th>Shetty et al. (CNN)</th>
<th>Image Learner (this tutorial)</th>
</tr>
<tr>
<td>Accuracy</td>
<td>0.94 (94%)</td>
<td>0.90 (90%)</td>
</tr>
<tr>
<td>Weighted Precision</td>
<td>0.88 (88%)</td>
<td>0.90 (90%)</td>
</tr>
<tr>
<td>Weighted Recall</td>
<td>0.85 (85%)</td>
<td>0.90 (90%)</td>
</tr>
<tr>
<td>Weighted F1-Score</td>
<td>0.86 (86%)</td>
<td>0.90 (90%)</td>
</tr>
</table>

<p>Image Learner shows slightly lower accuracy but higher weighted precision/recall/F1, reflecting balanced class-wise performance under the same evaluation protocol.</p>

<h2>Conclusion</h2>

<p>In this tutorial, we demonstrated how to use the Galaxy Image Learner tool to build a deep learning model for skin lesion classification using the HAM10000 dataset. We followed a structured approach consisting of:</p>

<ul>
<li>Uploading image datasets (ZIP files) and metadata (CSV)</li>
<li>Configuring Image Learner with appropriate hyperparameters for transfer learning</li>
<li>Training a deep learning classifier using the caformer_s18_384 model architecture</li>
<li>Evaluating the model's performance using accuracy and weighted precision/recall/F1 plus diagnostic plots</li>
<li>Interpreting results through visualizations (confusion matrix, learning curves, per-class metrics)</li>
</ul>

<p>Throughout the process, we showcased how Image Learner simplifies deep learning workflows, making them accessible to researchers without extensive coding expertise. The model achieved ~90% accuracy with balanced weighted precision/recall/F1, demonstrating strong performance for dermoscopic image classification.</p>

<p>By the end of this tutorial, you should have a solid understanding of how to:</p>
<ul>
<li>Prepare image datasets for deep learning</li>
<li>Configure transfer learning models for image classification</li>
<li>Evaluate and interpret deep learning model performance</li>
<li>Handle class-imbalanced datasets through augmentation</li>
<li>Apply deep learning to biomedical image classification tasks</li>
</ul>

<p>The approaches and insights from this tutorial can be generalized to other image classification tasks in biomedical research and beyond.</p>

<h2>Additional Resources</h2>

<ul>
<li><strong>Dataset</strong>: <a href="https://zenodo.org/records/17114688">HAM10000 on Zenodo</a></li>
<li><strong>Galaxy US Server</strong>: <a href="https://usegalaxy.org">https://usegalaxy.org</a></li>
<li><strong>Cancer Galaxy</strong>: <a href="https://cancer.usegalaxy.org">https://cancer.usegalaxy.org</a></li>
<li><strong>Galaxy Training Network</strong>: <a href="https://training.galaxyproject.org">https://training.galaxyproject.org</a></li>
</ul>

</body>
</html>
